## Config for Crafting BC IL experiment
env: mazebase-v0
env_kwargs:
  config: seen_full

eval_env: mazebase-v0
eval_env_kwargs:
  config: unseen_full

alg: BehaviorCloningForwardPrediction
alg_kwargs:
  dataset: datasets/new_data/dataset_
  validation_dataset: datasets/valid_new_data/dataset_
  dataset_fraction: 0.4 # Default example uses only 40% of the data, where we got the best results!
  batch_size: 64
  optim_cls: ["import", "torch.optim", "AdamW"]
  optim_kwargs:
    lr: 0.0001
    eps: 0.00000001
    weight_decay: 0.05
  grad_norm: 1.0
  action_coeff: 1.0
  forward_coeff: 0.25
  unsup_coeff: 0.0

train_kwargs:
  total_steps: 325000
  log_freq: 100
  eval_ep: 50
  eval_freq: 5000
  validation_metric: action_accuracy
  use_eval_mode: False
  max_eval_steps: 50

network: ViTForward
network_kwargs:
  depth: 4
  num_heads: 2
  embed_dim: 128
  unsup_dim: 128
  mlp_ratio: 2
  decoder_depth: 1
  drop_rate: 0.1
  attn_drop_rate: 0.1
  drop_path_rate: 0.1
